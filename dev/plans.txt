Make sure to use the agents. This project is about generative AI, and the idea is that (using the included code from the GitHub of DTgRPCconnector and ModuLLe, kept inside of the dev folder) it's possible to make something that connects to a vision enabled LLM and uses it for various tasks revolving around prompts, then it'll generate an image using the gRPC stuff. Keep all temporary, development and otherwise useless stuff for the end user, into the dev folder, neatly sorted in subfolders as needed, so that the gitignore file will be simple and lean. Let's use a venv for compatibility. The software will have some sort of interface (very much open for discussion how to do that!) with a zone for writing a prompt, a file selector to select one image (and a preview of said image), a few options for different tasks (still TBD what exactly), a zone for the output prompt of the LLM, a zone for the output of the generated image from the gRPC, and settings to allow the gRPC and LLM to be set up (separately). For testing purposes on the local machine, we will use (at 192.168.2.20:1234) LM Studio with the model qwen3-vl-8b-instruct-abliterated-v2.0 while the gRPC server is at 192.168.2.150:7859
Basic features would be "prompt expansion" where the user inputs a prompt and the LLM will "expand" on it. Another feature would be "prompt extraction" where the user selects an image and the LLM writes a prompt based on that. Then, a "prompt refiner" where the user will ask the LLM to modify things (i.e. "change the hair to red") and the LLM will rework the prompt as necessary. Also, a simple option to write the prompt and just use it as is. The user will always have a button to start a generation based on the available prompt. The basic settings need to be the address for the LLM and gRPC, both defaulting to localhost for simplicity, then a list of available LLM for the LLM component, and a list of models (plus one of lora) for the gRPC side of things. The gRPC will need a bunch of important settings, as different models require different settings! notably clip skip is quite important as many models either ignore it or work better with it set to 1... but many others NEED it to 2 or won't work at all. I guess we need a bunch of text files for ease of editing... I'll have to write some to use as system prompts for the LLM, and something as presets for different kinds of models.

The project's GitHub is https://github.com/Alexthestampede/MuddleMeThis
